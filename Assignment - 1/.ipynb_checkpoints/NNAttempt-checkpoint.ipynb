{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f50167a4",
   "metadata": {},
   "source": [
    "# *Loading and Splitting Datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0fae49f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a6eae59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ion_X = np.genfromtxt(\"ionosphere.txt\", delimiter = \",\", usecols = np.arange(34))\n",
    "ion_y = np.genfromtxt(\"ionosphere.txt\", delimiter = \",\", usecols = 34)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0c23ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ion_X_train, ion_X_test, ion_y_train, ion_y_test = train_test_split(ion_X, ion_y, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cbf7647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris['data'],iris['target'], random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8db849ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NearestNeighbor2(X_train, X_test, y_train, y_test):\n",
    "    err_no = 0 # error counter\n",
    "    nn_index = 0 # index of nearest neighbor\n",
    "    for i in range(len(X_test)): # check each test sample[*]\n",
    "        difference = 1000.0 # difference initialised to large number\n",
    "        for j in range (len(X_train)): # [*]against all training samples\n",
    "            # we take the difference between neighbors and add values of all features in this array together\n",
    "            # we then divide this number by the number of features in samples and take its abs value\n",
    "            # this final value is regarded as the difference between neighbors\n",
    "            sum_average = abs(sum(X_train[j] - X_test[i]) / len(X_train[j]))\n",
    "            # we find the NN and save its index\n",
    "            if difference > sum_average:\n",
    "                difference = sum_average\n",
    "                nn_index = j\n",
    "        # if the postulated label != actual label of the test sample there is an error\n",
    "        if y_train[nn_index] != y_test[i]: \n",
    "            err_no+=1\n",
    "    # we print the number of errors and their percentage compared to the total test samples\n",
    "    print(\"The number of errors of the training set on the test set is: \" + str(err_no))\n",
    "    print(\"test error rate: \" + str((err_no / len(X_test)) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bbd87e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of errors of the training set on the test set is: 23\n",
      "test error rate: 26.136363636363637%\n"
     ]
    }
   ],
   "source": [
    "NearestNeighbor2(ion_X_train, ion_X_test, ion_y_train, ion_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "29f923c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def NearestNeighbor1(X_train, X_test, y_train, y_test):\n",
    "    err_no = 0 # error counter\n",
    "    nn_index = 0 # index of nearest neighbor\n",
    "    for i in range(len(X_test)): # check each test sample[*]\n",
    "        difference = 1000.0 # difference initialised to large number\n",
    "        for j in range (len(X_train)): # [*]against all training samples\n",
    "            # find the distance between training sample and test sample\n",
    "            # we then find the euclidian norm of the value we are left with\n",
    "            distance = abs(X_train[j] - X_test[i])\n",
    "            euc_norm = 0\n",
    "            for f in distance:\n",
    "                euc_norm += f**2\n",
    "            euc_norm = sqrt(euc_norm)\n",
    "            # we find the NN and save its index\n",
    "            if difference > euc_norm:\n",
    "                difference = euc_norm\n",
    "                nn_index = j\n",
    "        # if the postulated label != actual label of the test sample there is an error\n",
    "        if y_train[nn_index] != y_test[i]: \n",
    "            err_no+=1\n",
    "    # we print the number of errors and their percentage compared to the total test samples\n",
    "    print(\"The number of errors of the training set on the test set is: \" + str(err_no))\n",
    "    print(\"test error rate: \" + str((err_no / len(X_test)) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "42f11099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of errors of the training set on the test set is: 13\n",
      "test error rate: 14.772727272727273%\n"
     ]
    }
   ],
   "source": [
    "NearestNeighbor1(ion_X_train, ion_X_test, ion_y_train, ion_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5d96df6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of errors of the training set on the test set is: 2\n",
      "test error rate: 5.263157894736842%\n"
     ]
    }
   ],
   "source": [
    "NearestNeighbor1(iris_X_train, iris_X_test, iris_y_train, iris_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6814b857",
   "metadata": {},
   "source": [
    "# Conformity Measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a614b",
   "metadata": {},
   "source": [
    "## First we break down each formula used and implement each of their components as functions\n",
    "\n",
    "### We take the conformity score, a, to be:\n",
    "### The distance to the nearest sample of a different class / the distance to the nearest sample of the same class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d763ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# this method returns the distance of the test sample to the nearest sample in the same class\n",
    "# the test sample parameter is a single sample\n",
    "def Nearest_Same(X_train, X_test, y_train, y_test):\n",
    "    distance = 1000.0 \n",
    "    for i in range(len(X_train)):\n",
    "        # we only want to compare to the training samples of the same label-class\n",
    "        if y_train[i] == y_test: \n",
    "            difference = abs(X_train[i] - X_test)\n",
    "            euc_norm = 0\n",
    "            for f in difference:\n",
    "                euc_norm += f**2\n",
    "            euc_norm = sqrt(euc_norm)\n",
    "            if distance > euc_norm:\n",
    "                distance = euc_norm\n",
    "    return distance\n",
    "\n",
    "# this method returns the distance of the test sample to the nearest sample in a different class\n",
    "# the test sample parameter is a single sample\n",
    "def Nearest_Different(X_train, X_test, y_train, y_test):\n",
    "    distance = 1000.0\n",
    "    for i in range(len(X_train)):\n",
    "         # we only want to compare to the training samples of a different label-class\n",
    "        if y_train[i] != y_test:\n",
    "            difference = abs(X_train[i] - X_test)\n",
    "            euc_norm = 0\n",
    "            for f in difference:\n",
    "                euc_norm += f**2\n",
    "            if euc_norm == 0:\n",
    "                break\n",
    "            euc_norm = sqrt(euc_norm)\n",
    "            if distance > euc_norm:\n",
    "                distance = euc_norm\n",
    "    return distance\n",
    "\n",
    "# conformity score function defined below\n",
    "def conformity_score(distance_different, distance_same):\n",
    "    return  distance_different / distance_same\n",
    "\n",
    "from bisect import bisect\n",
    "\n",
    "# the bisect function allows us to insert an item into a sorted list\n",
    "# and as return value gives its index, i.e. its conformity score rank\n",
    "def find_rank(scores, sample):\n",
    "    scores.sort()\n",
    "    rank = bisect(scores, sample)\n",
    "    # [rank + 1] is returned because arrays are 0-based\n",
    "    return rank + 1\n",
    "\n",
    "\n",
    "# we need to find the conformity score of each training sample to determine ranks for p-value\n",
    "def Conform_Score_Train(X_train, y_train):\n",
    "    scores = []\n",
    "    for i in range(len(X_train)):\n",
    "        # the training set without i-th sample\n",
    "        # label set without i-th samples label\n",
    "        X_no_i = np.delete(X_train, i, axis = 0)\n",
    "        y_no_i = np.delete(y_train, i, axis = 0)\n",
    "            \n",
    "        # to find the conformity score, we need to find distances to nearest same and nearest different\n",
    "        distance_same = Nearest_Same(X_no_i, X_train[i], y_no_i, y_train[i])\n",
    "        distance_different = Nearest_Different(X_no_i, X_train[i], y_no_i, y_train[i])\n",
    "        \n",
    "        scores.append(conformity_score(distance_different, distance_same))\n",
    "    #return conformity scores of triaining set for ranking\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "0471d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function implemented for the p-value formula\n",
    "def p_value(rank, number_samples):\n",
    "    return rank / number_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "21e47ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conformal_Predictor(X_train, X_test, y_train, y_test):\n",
    "    # this is the numerator for the p-value formula\n",
    "    # the number of training samples + test sample\n",
    "    n_plus_one = len(X_train) + 1\n",
    "    \n",
    "    # this variable is for the sum of all the false p-values\n",
    "    total_false = 0.0\n",
    "    \n",
    "    # we start by identifying classes\n",
    "    classes = np.unique(y_train)\n",
    "    \n",
    "    # an array containing all conformity scores of training set for ranking\n",
    "    conform_scores = Conform_Score_Train(X_train, y_train)\n",
    "    \n",
    "    # to find every false p-value of each test sample, we first consider test samples individually\n",
    "    for s in range (len(X_test)):\n",
    "        for c in range(len(classes)):\n",
    "            # we consider every class that our test sample is NOT in, i.e. false label\n",
    "            if y_test[s] != classes[c]:\n",
    "                # for the test sample, we assume its label is classes[c] to find false p-value\n",
    "                # first find distances for the nearest same and nearest different\n",
    "                test_nearest_same = Nearest_Same(X_train, X_test[s], y_train, classes[c])\n",
    "                test_nearest_different = Nearest_Different(X_train, X_test[s], y_train, c)\n",
    "                # since the nearest same is the numerator, we avoid zero-values\n",
    "                if test_nearest_same != 0:\n",
    "                    # find conformity score of the test sample\n",
    "                    test_conform_score = conformity_score(test_nearest_different, test_nearest_same)\n",
    "                    # then find rank to find false p-value\n",
    "                    rank = find_rank(conform_scores, test_conform_score)\n",
    "                    # add false p-value to the total\n",
    "                    total_false += p_value(rank, n_plus_one)\n",
    "    # average false is the total false p-values, divided by the number of total possible false p-values\n",
    "    # this value is calculated as the number of test samples\n",
    "    average_false = total_false / (len(X_test) * (len(classes) -1))\n",
    "    \n",
    "    return \"The average false p-values for this test sample-set is: \" + str(average_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "3809bf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The average false p-values for this test sample-set is: 0.06628787878787883'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conformal_Predictor(ion_X_train, ion_X_test, ion_y_train, ion_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08443d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0675cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
