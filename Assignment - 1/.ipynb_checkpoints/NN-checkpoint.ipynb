{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e834b8e",
   "metadata": {},
   "source": [
    "# *Loading and Splitting Datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1690d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bd334e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ion_X = np.genfromtxt(\"ionosphere.txt\", delimiter = \",\", usecols = np.arange(34))\n",
    "ion_y = np.genfromtxt(\"ionosphere.txt\", delimiter = \",\", usecols = 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07404906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# random_state = 13 bc my bDay\n",
    "ion_X_train, ion_X_test, ion_y_train, ion_y_test = train_test_split(ion_X, ion_y, random_state = 13)\n",
    "iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris['data'],iris['target'], random_state = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a9661e",
   "metadata": {},
   "source": [
    "# *Nearest Neighbor Method implementation*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29cc64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def NearestNeighbor(X_train, X_test, y_train, y_test):\n",
    "    err_no = 0 # error counter\n",
    "    nn_index = 0 # index of nearest neighbor\n",
    "    \n",
    "    for i in range(len(X_test)): # check each test sample[*]\n",
    "        \n",
    "        difference = 1000.0 # difference between two samples initialised to large number\n",
    "        \n",
    "        for j in range (len(X_train)): # [*] against all training samples\n",
    "            \n",
    "            # find the distance between training sample and test sample\n",
    "            distance = abs(X_train[j] - X_test[i])\n",
    "            euc_norm = 0\n",
    "            for f in distance:\n",
    "                euc_norm += f**2\n",
    "            # we then find the euclidian norm of the value we are left with\n",
    "            euc_norm = sqrt(euc_norm)\n",
    "            \n",
    "            # we find the NN and save its index\n",
    "            if difference > euc_norm:\n",
    "                difference = euc_norm\n",
    "                nn_index = j\n",
    "                \n",
    "        # if the predicted label != actual label of the test sample there is an error\n",
    "        if y_train[nn_index] != y_test[i]: \n",
    "            err_no+=1\n",
    "            \n",
    "    # we print the number of errors and their percentage compared to the total test samples\n",
    "    print(\"The number of errors of the training set on the test set is: \" + str(err_no))\n",
    "    print(\"test error rate: \" + str((err_no / len(X_test)) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795789f8",
   "metadata": {},
   "source": [
    "# *Error rates of the datasets*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e630df",
   "metadata": {},
   "source": [
    "### running the Nearest Neighbor method on the ionosphere.txt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed9645cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of errors of the training set on the test set is: 13\n",
      "test error rate: 14.772727272727273%\n"
     ]
    }
   ],
   "source": [
    "NearestNeighbor(ion_X_train, ion_X_test, ion_y_train, ion_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aebaf6",
   "metadata": {},
   "source": [
    "### running the Nearest Neighbor method on the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2545253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of errors of the training set on the test set is: 2\n",
      "test error rate: 5.263157894736842%\n"
     ]
    }
   ],
   "source": [
    "NearestNeighbor(iris_X_train, iris_X_test, iris_y_train, iris_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3638bf",
   "metadata": {},
   "source": [
    "# *Conformal Predictor: 1-NN Conformity Measure*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b98c4",
   "metadata": {},
   "source": [
    "## *First we break down each formula used, and implement each of their components as functions*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1456ffac",
   "metadata": {},
   "source": [
    "### We take the conformity score, a, to be:\n",
    "### *The distance to the nearest sample of a different class / the distance to the nearest sample of the same class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef5fdf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method returns the distance of the test sample to the nearest sample in the same class\n",
    "# the test sample parameter is a single sample\n",
    "def Nearest_Same(X_train, X_test, y_train, y_test):\n",
    "    distance = 1000.0 \n",
    "    for i in range(len(X_train)):\n",
    "        # we only want to compare to the training samples of the same label-class\n",
    "        if y_train[i] == y_test: \n",
    "            difference = abs(X_train[i] - X_test)\n",
    "            euc_norm = 0\n",
    "            for f in difference:\n",
    "                euc_norm += f**2\n",
    "            euc_norm = sqrt(euc_norm)\n",
    "            if distance > euc_norm:\n",
    "                distance = euc_norm\n",
    "    return distance\n",
    "\n",
    "# this method returns the distance of the test sample to the nearest sample in a different class\n",
    "# the test sample parameter is a single sample\n",
    "def Nearest_Different(X_train, X_test, y_train, y_test):\n",
    "    distance = 1000.0\n",
    "    for i in range(len(X_train)):\n",
    "         # we only want to compare to the training samples of a different label-class\n",
    "        if y_train[i] != y_test:\n",
    "            difference = abs(X_train[i] - X_test)\n",
    "            euc_norm = 0\n",
    "            for f in difference:\n",
    "                euc_norm += f**2\n",
    "            if euc_norm == 0:\n",
    "                break\n",
    "            euc_norm = sqrt(euc_norm)\n",
    "            if distance > euc_norm:\n",
    "                distance = euc_norm\n",
    "    return distance\n",
    "\n",
    "# conformity score function defined below\n",
    "def conformity_score(distance_different, distance_same):\n",
    "    return  distance_different / distance_same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e542b5",
   "metadata": {},
   "source": [
    "### *For the p-value formula we need to find the rank of the conformity score, we implement it as a method*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "850eb676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect\n",
    "\n",
    "# the bisect function allows us to insert an item into a sorted list\n",
    "# and as return value gives its index, i.e. its conformity score rank\n",
    "def find_rank(scores, sample):\n",
    "    scores.sort()\n",
    "    rank = bisect(scores, sample)\n",
    "    # [rank + 1] is returned because arrays are 0-based\n",
    "    return rank + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55631a9f",
   "metadata": {},
   "source": [
    "### *finally, we implement the p-value formula*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9781f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function implemented for the p-value formula\n",
    "def p_value(rank, number_samples):\n",
    "    return rank / number_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68c59b0",
   "metadata": {},
   "source": [
    "### *we refactor code and implement a function which generates a list of conformity scores for the training set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd9c08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to find the conformity score of each training sample to determine ranks for p-value\n",
    "def Conform_Score_Train(X_train, y_train):\n",
    "    scores = []\n",
    "    for i in range(len(X_train)):\n",
    "        # the training set without i-th sample\n",
    "        # label set without i-th samples label\n",
    "        X_no_i = np.delete(X_train, i, axis = 0)\n",
    "        y_no_i = np.delete(y_train, i, axis = 0)\n",
    "            \n",
    "        # to find the conformity score, we need to find distances to nearest same and nearest different\n",
    "        distance_same = Nearest_Same(X_no_i, X_train[i], y_no_i, y_train[i])\n",
    "        distance_different = Nearest_Different(X_no_i, X_train[i], y_no_i, y_train[i])\n",
    "        \n",
    "        scores.append(conformity_score(distance_different, distance_same))\n",
    "    #return conformity scores of triaining set for ranking\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb7905",
   "metadata": {},
   "source": [
    "### *We implement the conformal predictor function, which finds all the false p-values of a test-set, and returns the average of their sum*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3ddbe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conformal_Predictor(X_train, X_test, y_train, y_test):\n",
    "    # this is the numerator for the p-value formula\n",
    "    # the number of training samples + test sample\n",
    "    n_plus_one = len(X_train) + 1\n",
    "    \n",
    "    # this variable is for the sum of all the false p-values\n",
    "    total_false = 0.0\n",
    "    \n",
    "    # we start by identifying classes\n",
    "    classes = np.unique(y_train)\n",
    "    \n",
    "    # an array containing all conformity scores of training set for ranking\n",
    "    conform_scores = Conform_Score_Train(X_train, y_train)\n",
    "    \n",
    "    # to find every false p-value of each test sample, we first consider test samples individually\n",
    "    for s in range (len(X_test)):\n",
    "        for c in range(len(classes)):\n",
    "            # we consider every class that our test sample is NOT in, i.e. false label\n",
    "            if y_test[s] != classes[c]:\n",
    "                # for the test sample, we assume its label is classes[c] to find false p-value\n",
    "                # first find distances for the nearest same and nearest different\n",
    "                test_nearest_same = Nearest_Same(X_train, X_test[s], y_train, classes[c])\n",
    "                test_nearest_different = Nearest_Different(X_train, X_test[s], y_train, c)\n",
    "                # since the nearest same is the numerator, we avoid zero-values\n",
    "                if test_nearest_same != 0:\n",
    "                    # find conformity score of the test sample\n",
    "                    test_conform_score = conformity_score(test_nearest_different, test_nearest_same)\n",
    "                    # then find rank to find false p-value\n",
    "                    rank = find_rank(conform_scores, test_conform_score)\n",
    "                    # add false p-value to the total\n",
    "                    total_false += p_value(rank, n_plus_one)\n",
    "    # average false is the total false p-values, divided by the number of total possible false p-values\n",
    "    # this value is calculated as the number of test samples\n",
    "    average_false = total_false / (len(X_test) * (len(classes) -1))\n",
    "    \n",
    "    return \"The average false p-values for this test sample-set is: \" + str(average_false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9bca18",
   "metadata": {},
   "source": [
    "# *Average False P-values of the datasets*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1afeb",
   "metadata": {},
   "source": [
    "## *Average false p-value for the ionosphere.txt dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8803c2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The average false p-values for this test sample-set is: 0.06628787878787883'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conformal_Predictor(ion_X_train, ion_X_test, ion_y_train, ion_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e63570f",
   "metadata": {},
   "source": [
    "## *Average false p-value for the iris dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12ae79f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The average false p-values for this test sample-set is: 0.012342803912435968'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conformal_Predictor(iris_X_train, iris_X_test, iris_y_train, iris_y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
